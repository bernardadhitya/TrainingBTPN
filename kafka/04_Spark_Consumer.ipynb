{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Consumer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbfmGHjyT7ac"
      },
      "source": [
        "\n",
        "# Spark Consumer\n",
        "\n",
        "\n",
        "## Install Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqsWrkiNK4N0"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgLl4h7HUQpW"
      },
      "source": [
        "## Set java and spark home"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhy_p_s_LAP6"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIcIC23EUUB-"
      },
      "source": [
        "## Install Kafka Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR4bbQqmLGqv"
      },
      "source": [
        "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.4.7/spark-sql-kafka-0-10_2.11-2.4.7.jar\n",
        "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.4.7/spark-streaming-kafka-0-10-assembly_2.11-2.4.7.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rze2PfAdUXfx"
      },
      "source": [
        "## Add Kafka Dependecies to spark shell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSOKnKMXLHcR"
      },
      "source": [
        "import os\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-sql-kafka-0-10_2.11-2.4.7.jar pyspark-shell'\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-10-assembly_2.11-2.4.7.jar,/content/spark-sql-kafka-0-10_2.11-2.4.7.jar pyspark-shell'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uStXLtKqUesV"
      },
      "source": [
        "## Initialize Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MQEFZMFLTef"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stfczWRQPLZ7"
      },
      "source": [
        "from pyspark.sql.functions import from_json, expr\n",
        "from pyspark.sql.types import StructType, StructField, TimestampType, LongType, DoubleType, StringType\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import time\n",
        "\n",
        "import chartify\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afMzOLqGUhGU"
      },
      "source": [
        "## Declare JSON Schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woR4-4oFPUF8"
      },
      "source": [
        "schema = StructType([\n",
        "    StructField(\"start\", TimestampType(), True), \n",
        "    StructField(\"end\", TimestampType(), True),\n",
        "    StructField(\"count\", LongType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"lon\", DoubleType(), True),\n",
        "    StructField(\"locationName\", StringType(), True)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ6NojgaUkeu"
      },
      "source": [
        "## Set kafka variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URJpuDYIPaQ8"
      },
      "source": [
        "username = \"pujo\"\n",
        "server_ip = \"34.87.150.250\"\n",
        "bootstrap_servers = f\"{server_ip}:9092,{server_ip}:9093,{server_ip}:9094\"\n",
        "schema_registry_url = f\"http://{server_ip}:8081\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGU1Kw2uUpoJ"
      },
      "source": [
        "## Create Kafka Consumer to Console"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkrObS0DPYco"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, expr\n",
        "from pyspark.sql.types import StructType, StructField, TimestampType, LongType, IntegerType, StringType\n",
        "import time\n",
        "\n",
        "\n",
        "class KafkaToConsoleApp:\n",
        "    \"\"\"\n",
        "    The KafkaToConsoleApp reads records from a Kafka topic and shows them on the console.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, processing_time):\n",
        "        self.spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "        print(\"Spark version is: %s\" % self.spark.version)\n",
        "        print(self.spark.sparkContext.getConf().getAll())\n",
        "        self.processingTime = processing_time\n",
        "\n",
        "    @staticmethod\n",
        "    def write_micro_batch(micro_batch_df, batch_id):\n",
        "        ts = time.localtime()\n",
        "        print(\"Showing batch %s at %s\" % (batch_id, time.strftime(\"%Y-%m-%d %H:%M:%S\", ts)))\n",
        "        micro_batch_df.orderBy(\"ordinal\").show(truncate=False)\n",
        "\n",
        "    def load(self, output_mode):\n",
        "        self.get_events_df().writeStream \\\n",
        "            .outputMode(output_mode) \\\n",
        "            .trigger(processingTime=self.processingTime) \\\n",
        "            .foreachBatch(self.write_micro_batch) \\\n",
        "            .start()\n",
        "        self.spark.streams.awaitAnyTermination()\n",
        "\n",
        "    def get_events_df(self):\n",
        "        schema = StructType([\n",
        "            StructField(\"ordinal\", LongType(), True),\n",
        "            StructField(\"locationId\", IntegerType(), True),\n",
        "            StructField(\"timestamp\", TimestampType(), True),\n",
        "            StructField(\"amount\", LongType(), True),\n",
        "        ])\n",
        "\n",
        "        # The events are watermarked on the eventTimestamp custom field (not the kafka timestamp)\n",
        "        # Delay threshold indicates how much time the system will wait for new events based on the watermark\n",
        "        return self.spark.readStream \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
        "            .option(\"subscribe\", f\"{username}-spark-events\") \\\n",
        "            .load() \\\n",
        "            .withColumn(\"key\", expr(\"string(key)\")) \\\n",
        "            .withColumn(\"value\", from_json(expr(\"string(value)\"), schema)) \\\n",
        "            .withColumn(\"ordinal\", expr(\"value.ordinal\")) \\\n",
        "            .withColumn(\"locationId\", expr(\"value.locationId\")) \\\n",
        "            .withColumn(\"eventTimestamp\", expr(\"value.timestamp\")) \\\n",
        "            .withColumn(\"amount\", expr(\"value.amount\")) \\\n",
        "            .withWatermark(eventTime=\"eventTimestamp\", delayThreshold=\"30 seconds\") \\\n",
        "            .drop(\"value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG_e_nGwQkT6"
      },
      "source": [
        "x = KafkaToConsoleApp(processing_time=\"10 seconds\")\n",
        "x.load(output_mode=\"append\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4F17gFAUvGH"
      },
      "source": [
        "## Count Event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcLR6iUUQ99x"
      },
      "source": [
        "from pyspark.sql.functions import window\n",
        "import time\n",
        "\n",
        "class KafkaEventCountApp(KafkaToConsoleApp):\n",
        "    \"\"\"\n",
        "    The KafkaEventCountApp reads records from a Kafka topic, calculates the aggregate count grouped by time window and\n",
        "    shows the result on the screen\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def write_micro_batch(micro_batch_df, batch_id):\n",
        "        ts = time.localtime()\n",
        "        print(\"Showing batch: %s, at %s\" % (batch_id, time.strftime(\"%Y-%m-%d %H:%M:%S\", ts)))\n",
        "        micro_batch_df.orderBy(micro_batch_df[\"window.start\"]).show(truncate=False)\n",
        "\n",
        "    def get_events_df(self):\n",
        "        events_df = super().get_events_df()\n",
        "        count_df = events_df.groupBy(window(events_df[\"eventTimestamp\"], \"60 seconds\")).count()\n",
        "        return count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrhSDzhcRMSa"
      },
      "source": [
        "x = KafkaEventCountApp(processing_time=\"10 seconds\")\n",
        "x.load(\"complete\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}