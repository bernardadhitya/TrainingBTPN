{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Producer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7uTwr1PJoOl"
      },
      "source": [
        "\n",
        "# Spark Producer\n",
        "\n",
        "\n",
        "## Install Spark\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cchMOSfI9UyY"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggbsc0ALJ0q9"
      },
      "source": [
        "## Set java and spark home"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dppywdu5-reQ"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxgB9d_4J6_Z"
      },
      "source": [
        "## Install Kafka Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1uQ5_80CArQ"
      },
      "source": [
        "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.4.7/spark-sql-kafka-0-10_2.11-2.4.7.jar\n",
        "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.11/2.4.7/spark-streaming-kafka-0-10-assembly_2.11-2.4.7.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xauej_3gKBTt"
      },
      "source": [
        "## Add Kafka Dependecies to spark shell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux9NswnzCjH9"
      },
      "source": [
        "import os\n",
        "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-sql-kafka-0-10_2.11-2.4.7.jar pyspark-shell'\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-10-assembly_2.11-2.4.7.jar,/content/spark-sql-kafka-0-10_2.11-2.4.7.jar pyspark-shell'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPIMZsdbKFb6"
      },
      "source": [
        "## Initialize Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnKBNd-S-1w-"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7C6aZM8KLUE"
      },
      "source": [
        "## Create Data Producer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZGyPRxT-4uM"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import struct, to_json, expr\n",
        "import time\n",
        "\n",
        "\n",
        "class RateToConsoleApp:\n",
        "    \"\"\"\n",
        "    The RateToConsoleApp reads records from a Apache Spark rate (fake) stream and shows them in the console.\n",
        "    Useful to emulate events.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, processing_time):\n",
        "        self.spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "        print(\"Spark version is: %s\" % self.spark.version)\n",
        "        print(self.spark.sparkContext.getConf().getAll())\n",
        "        self.processingTime = processing_time\n",
        "\n",
        "    @staticmethod\n",
        "    def write_micro_batch(micro_batch_df, batch_id):\n",
        "        ts = time.localtime()\n",
        "        print(\"Showing batch %s at %s\" % (batch_id, time.strftime(\"%Y-%m-%d %H:%M:%S\", ts)))\n",
        "        micro_batch_df.show(truncate=False)\n",
        "\n",
        "    def load(self, output_mode):\n",
        "        events_df = self.get_events_df()\n",
        "\n",
        "        events_df.writeStream \\\n",
        "            .outputMode(output_mode) \\\n",
        "            .trigger(processingTime=self.processingTime) \\\n",
        "            .foreachBatch(self.write_micro_batch) \\\n",
        "            .start()\n",
        "\n",
        "        self.spark.streams.awaitAnyTermination()\n",
        "\n",
        "    def get_events_df(self):\n",
        "        rate_df = self.spark.readStream.format(\"rate\").load()\n",
        "        events_df = rate_df \\\n",
        "            .withColumn(\"key\", expr(\"uuid()\")) \\\n",
        "            .withColumn(\"value\",\n",
        "                        to_json(struct(rate_df[\"value\"].alias(\"ordinal\"),\n",
        "                                       expr(\"value % 3 +1\").alias(\"locationId\"),\n",
        "                                       rate_df[\"timestamp\"],\n",
        "                                       expr(\"floor(rand() * 100000000 / 100)\").alias(\"amount\")))) \\\n",
        "            .select(\"key\", \"value\")\n",
        "\n",
        "        return events_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGbatOYx_za8"
      },
      "source": [
        "x = RateToConsoleApp('5 seconds')\n",
        "x.load(\"append\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSz1FPvfKRVJ"
      },
      "source": [
        "## Set kafka variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p8F8lTfApmT"
      },
      "source": [
        "username = \"pujo\"\n",
        "server_ip = \"34.87.150.250\"\n",
        "bootstrap_servers = f\"{server_ip}:9092,{server_ip}:9093,{server_ip}:9094\"\n",
        "schema_registry_url = f\"http://{server_ip}:8081\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwmpgik6KX-E"
      },
      "source": [
        "## Create Kafka Producer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9a07NRMAa1C"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "class RateToKafkaApp(RateToConsoleApp):\n",
        "    \"\"\"\n",
        "    The RateToConsoleApp reads records from a Apache Spark rate (fake) stream and writes them to an Apache Kafka topic.\n",
        "    Useful to emulate events and send them to Kafka.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def write_micro_batch(micro_batch_df, batch_id):\n",
        "        ts = time.localtime()\n",
        "        print(\"Writting batch %s to kafka, at %s\" % (batch_id, time.strftime(\"%Y-%m-%d %H:%M:%S\", ts)))\n",
        "        micro_batch_df.show(truncate=False)\n",
        "        micro_batch_df.write \\\n",
        "            .format(\"kafka\") \\\n",
        "            .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
        "            .option(\"topic\", f\"{username}-spark-events\") \\\n",
        "            .save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qszBSM9GKXbL"
      },
      "source": [
        "## Produce data to kafka"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhSzuwMHA24k"
      },
      "source": [
        "x = RateToKafkaApp(processing_time=\"5 seconds\")\n",
        "x.load(\"append\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-NJKRDPJlMX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}