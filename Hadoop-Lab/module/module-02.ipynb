{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# <center> Introduction to Hadoop MapReduce </center>"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. Debugging Hadoop MapReduce Jobs"}, {"cell_type": "markdown", "metadata": {}, "source": "** Data: Movie Ratings and Recommendation **\n\nAn independent movie company is looking to invest in a new movie project. With limited finance, the company wants to \nanalyze the reaction of audiences, particularly toward various movie genres, in order to identify beneficial \nmovie project to focus on. The company relies on data collected from a publicly available recommendation service \nby [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This \n[dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) contains **24404096** ratings and **668953**\n tag applications across **40110** movies. These data were created by **247753** users between January 09, 1995 and January 29, 2016. This dataset was generated on October 17, 2016. \n\nFrom this dataset, several analyses are possible, include the followings:\n1.   Find movies which have the highest average ratings over the years and identify the corresponding genre.\n2.   Find genres which have the highest average ratings over the years.\n3.   Find users who rate movies most frequently in order to contact them for in-depth marketing analysis.\n\nThese types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amount of data in \nelatively short amount of time for decision support purposes. In these situations, the sizes of the data typically \nmake analysis done on a single machine impossible and analysis done using a remote storage system impractical. For \nremainder of the lessons, we will learn how HDFS provides the basis to store massive amount of data and to enable \nthe programming approach to analyze these data."}, {"cell_type": "code", "execution_count": 2, "metadata": {"scrolled": true, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 7 items\n-rw-r--r--   2 nurcahyopujo hadoop     10.2 K 2021-04-26 23:26 /data_training/README.txt\n-rw-r--r--   2 nurcahyopujo hadoop    415.0 M 2021-04-26 23:27 /data_training/genome-scores.csv\n-rw-r--r--   2 nurcahyopujo hadoop     17.7 K 2021-04-26 23:26 /data_training/genome-tags.csv\n-rw-r--r--   2 nurcahyopujo hadoop      1.3 M 2021-04-26 23:26 /data_training/links.csv\n-rw-r--r--   2 nurcahyopujo hadoop      2.9 M 2021-04-26 23:26 /data_training/movies.csv\n-rw-r--r--   2 nurcahyopujo hadoop    646.8 M 2021-04-26 23:27 /data_training/ratings.csv\n-rw-r--r--   2 nurcahyopujo hadoop     37.0 M 2021-04-26 23:26 /data_training/tags.csv\n"}], "source": "!hdfs dfs -ls -h /data_training"}, {"cell_type": "markdown", "metadata": {}, "source": "### Find movies which have the highest average ratings over the years and report their ratings and genres\n\n- Find the average ratings of all movies over the years\n- Sort the average ratings from highest to lowest\n- Report the results, augmented by genres"}, {"cell_type": "code", "execution_count": 3, "metadata": {"scrolled": true, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 7 items\n-rw-r--r--   2 nurcahyopujo hadoop      10460 2021-04-26 23:26 /data_training/README.txt\n-rw-r--r--   2 nurcahyopujo hadoop  435164157 2021-04-26 23:27 /data_training/genome-scores.csv\n-rw-r--r--   2 nurcahyopujo hadoop      18103 2021-04-26 23:26 /data_training/genome-tags.csv\n-rw-r--r--   2 nurcahyopujo hadoop    1368578 2021-04-26 23:26 /data_training/links.csv\n-rw-r--r--   2 nurcahyopujo hadoop    3038099 2021-04-26 23:26 /data_training/movies.csv\n-rw-r--r--   2 nurcahyopujo hadoop  678260987 2021-04-26 23:27 /data_training/ratings.csv\n-rw-r--r--   2 nurcahyopujo hadoop   38810332 2021-04-26 23:26 /data_training/tags.csv\n"}], "source": "!hdfs dfs -ls /data_training"}, {"cell_type": "code", "execution_count": 4, "metadata": {"scrolled": true, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Summary\n=======\n\nThis dataset (ml-25m) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 25000095 ratings and 1093360 tag applications across 62423 movies. These data were created by 162541 users between January 09, 1995 and November 21, 2019. This dataset was generated on November 21, 2019.\n\nUsers were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n\nThe data are contained in the files `genome-scores.csv`, `genome-tags.csv`, `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n\nThis and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/>.\n\n\nUsage License\n=============\n\nNeither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions:\n\n* The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group.\n* The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information).\n* The user may not redistribute the data without separate permission.\n* The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota.\n* The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction.\n\nIn no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).\n\nIf you have any further questions or comments, please email <grouplens-info@umn.edu>\n\n\nCitation\n========\n\nTo acknowledge use of the dataset in publications, please cite the following paper:\n\n> F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1\u201319:19. <https://doi.org/10.1145/2827872>\n\n\nFurther Information About GroupLens\n===================================\n\nGroupLens is a research group in the Department of Computer Science and Engineering at the University of Minnesota. Since its inception in 1992, GroupLens's research projects have explored a variety of fields including:\n\n* recommender systems\n* online communities\n* mobile and ubiquitious technologies\n* digital libraries\n* local geographic information systems\n\nGroupLens Research operates a movie recommender based on collaborative filtering, MovieLens, which is the source of these data. We encourage you to visit <http://movielens.org> to try it out! If you have exciting ideas for experimental work to conduct on MovieLens, send us an email at <grouplens-info@cs.umn.edu> - we are always interested in working with external collaborators.\n\n\nContent and Use of Files\n========================\n\nVerifying the Dataset Contents\n------------------------------\n\nWe encourage you to verify that the dataset you have on your computer is identical to the ones hosted at [grouplens.org](http://grouplens.org).  This is an important step if you downloaded the dataset from a location other than [grouplens.org](http://grouplens.org), or if you wish to publish research results based on analysis of the MovieLens dataset.\n\nWe provide a [MD5 checksum](http://en.wikipedia.org/wiki/Md5sum) with the same name as the downloadable `.zip` file, but with a `.md5` file extension. To verify the dataset:\n\n    # on linux\n    md5sum ml-25m.zip; cat ml-25m.zip.md5\n\n    # on OSX\n    md5 ml-25m.zip; cat ml-25m.zip.md5\n\n    # windows users can download a tool from Microsoft (or elsewhere) that verifies MD5 checksums\n\nCheck that the two lines of output contain the same hash value.\n\n\nFormatting and Encoding\n-----------------------\n\nThe dataset files are written as [comma-separated values](http://en.wikipedia.org/wiki/Comma-separated_values) files with a single header row. Columns that contain commas (`,`) are escaped using double-quotes (`\"`). These files are encoded as UTF-8. If accented characters in movie titles or tag values (e.g. Mis\u00e9rables, Les (1995)) display incorrectly, make sure that any program reading the data, such as a text editor, terminal, or script, is configured for UTF-8.\n\n\nUser Ids\n--------\n\nMovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between `ratings.csv` and `tags.csv` (i.e., the same id refers to the same user across the two files).\n\n\nMovie Ids\n---------\n\nOnly movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id `1` corresponds to the URL <https://movielens.org/movies/1>). Movie ids are consistent between `ratings.csv`, `tags.csv`, `movies.csv`, and `links.csv` (i.e., the same id refers to the same movie across these four data files).\n\n\nRatings Data File Structure (ratings.csv)\n-----------------------------------------\n\nAll ratings are contained in the file `ratings.csv`. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:\n\n    userId,movieId,rating,timestamp\n\nThe lines within this file are ordered first by userId, then, within user, by movieId.\n\nRatings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n\nTimestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n\n\nTags Data File Structure (tags.csv)\n-----------------------------------\n\nAll tags are contained in the file `tags.csv`. Each line of this file after the header row represents one tag applied to one movie by one user, and has the following format:\n\n    userId,movieId,tag,timestamp\n\nThe lines within this file are ordered first by userId, then, within user, by movieId.\n\nTags are user-generated metadata about movies. Each tag is typically a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\n\nTimestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n\n\nMovies Data File Structure (movies.csv)\n---------------------------------------\n\nMovie information is contained in the file `movies.csv`. Each line of this file after the header row represents one movie, and has the following format:\n\n    movieId,title,genres\n\nMovie titles are entered manually or imported from <https://www.themoviedb.org/>, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n\nGenres are a pipe-separated list, and are selected from the following:\n\n* Action\n* Adventure\n* Animation\n* Children's\n* Comedy\n* Crime\n* Documentary\n* Drama\n* Fantasy\n* Film-Noir\n* Horror\n* Musical\n* Mystery\n* Romance\n* Sci-Fi\n* Thriller\n* War\n* Western\n* (no genres listed)\n\n\nLinks Data File Structure (links.csv)\n---------------------------------------\n\nIdentifiers that can be used to link to other sources of movie data are contained in the file `links.csv`. Each line of this file after the header row represents one movie, and has the following format:\n\n    movieId,imdbId,tmdbId\n\nmovieId is an identifier for movies used by <https://movielens.org>. E.g., the movie Toy Story has the link <https://movielens.org/movies/1>.\n\nimdbId is an identifier for movies used by <http://www.imdb.com>. E.g., the movie Toy Story has the link <http://www.imdb.com/title/tt0114709/>.\n\ntmdbId is an identifier for movies used by <https://www.themoviedb.org>. E.g., the movie Toy Story has the link <https://www.themoviedb.org/movie/862>.\n\nUse of the resources listed above is subject to the terms of each provider.\n\n\nTag Genome (genome-scores.csv and genome-tags.csv)\n-------------------------------------------------\n\nThis data set includes a current copy of the Tag Genome.\n\n[genome-paper]: http://files.grouplens.org/papers/tag_genome.pdf\n\nThe tag genome is a data structure that contains tag relevance scores for movies.  The structure is a dense matrix: each movie in the genome has a value for *every* tag in the genome.\n\nAs described in [this article][genome-paper], the tag genome encodes how strongly movies exhibit particular properties represented by tags (atmospheric, thought-provoking, realistic, etc.). The tag genome was computed using a machine learning algorithm on user-contributed content including tags, ratings, and textual reviews.\n\nThe genome is split into two files.  The file `genome-scores.csv` contains movie-tag relevance data in the following format:\n\n    movieId,tagId,relevance\n\nThe second file, `genome-tags.csv`, provides the tag descriptions for the tag IDs in the genome file, in the following format:\n\n    tagId,tag\n\nThe `tagId` values are generated when the data set is exported, so they may vary from version to version of the MovieLens data sets.\n\nPlease include the following citation if referencing tag genome data:\n\n> Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3: 13:1\u201313:44. <https://doi.org/10.1145/2362394.2362395>\n\n\nCross-Validation\n----------------\n\nPrior versions of the MovieLens dataset included either pre-computed cross-folds or scripts to perform this computation. We no longer bundle either of these features with the dataset, since most modern toolkits provide this as a built-in feature. If you wish to learn about standard approaches to cross-fold computation in the context of recommender systems evaluation, see [LensKit](http://lenskit.org) for tools, documentation, and open-source code examples.\n"}], "source": "!hdfs dfs -cat /data_training/README.txt"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/links.csv \\\n    2>/dev/null | head -n 5"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/movies.csv \\\n    2>/dev/null | head -n 5"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv \\\n    2>/dev/null | head -n 5"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/tags.csv \\\n    2>/dev/null | head -n 5"}, {"cell_type": "markdown", "metadata": {}, "source": "### Note:\n\nTo write a MapReduce program, you have to be able to identify the necessary (Key,Value) that can contribute to the final realization of the required results. This is the reducing phase. From this (Key,Value) pair format, you will be able to develop the mapping phase. "}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile codes/avgRatingMapper01.py\n#!/usr/bin/env python\n\nimport sys\n\nfor oneMovie in sys.stdin:\n    oneMovie = oneMovie.strip()\n    ratingInfo = oneMovie.split(\",\")\n    movieID = ratingInfo[1]\n    rating = ratingInfo[2]\n    print (\"%s\\t%s\" % (movieID, rating)) "}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv \\\n    2>/dev/null | head -n 5 | python ./codes/avgRatingMapper01.py"}, {"cell_type": "markdown", "metadata": {}, "source": "#### *Do we really need the headers?*"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile codes/avgRatingMapper02.py\n#!/usr/bin/env python\n\nimport sys\n\nfor oneMovie in sys.stdin:\n    oneMovie = oneMovie.strip()\n    ratingInfo = oneMovie.split(\",\")\n    try:\n        movieID = ratingInfo[1]\n        rating = float(ratingInfo[2])\n        print (\"%s\\t%s\" % (movieID, rating))\n    except ValueError:\n        continue"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv \\\n    2>/dev/null | head -n 5 | python ./codes/avgRatingMapper02.py"}, {"cell_type": "markdown", "metadata": {}, "source": "#### *The outcome is correct. Is it useful?*"}, {"cell_type": "markdown", "metadata": {}, "source": "Getting additional file"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "!mkdir movielens\n!hdfs dfs -get /data_training/movies.csv movielens/movies.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile codes/avgRatingMapper03.py\n#!/usr/bin/env python\n\nimport sys\nimport csv\n\nmovieFile = \"./movielens/movies.csv\"\nmovieList = {}\n\nwith open(movieFile, mode = 'r') as infile:\n    reader = csv.reader(infile)\n    for row in reader:\n        movieList[row[0]] = {}\n        movieList[row[0]][\"title\"] = row[1]\n        movieList[row[0]][\"genre\"] = row[2]\n\nfor oneMovie in sys.stdin:\n    oneMovie = oneMovie.strip()\n    ratingInfo = oneMovie.split(\",\")\n    try:\n        movieTitle = movieList[ratingInfo[1]][\"title\"]\n        movieGenre = movieList[ratingInfo[1]][\"genre\"]\n        rating = float(ratingInfo[2])\n        print (\"%s\\t%s\\t%s\" % (movieTitle, rating, movieGenre))\n    except ValueError:\n        continue"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv \\\n    2>/dev/null | head -n 5 | python ./codes/avgRatingMapper03.py"}, {"cell_type": "markdown", "metadata": {}, "source": "#### *Test reducer:*"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile codes/avgRatingReducer01.py\n#!/usr/bin/env python\nimport sys\n\ncurrent_movie = None\ncurrent_rating_sum = 0\ncurrent_rating_count = 0\n\nfor line in sys.stdin:\n    line = line.strip()\n    movie, rating, genre = line.split(\"\\t\", 2)\n    try:\n        rating = float(rating)\n    except ValueError:\n        continue\n\n    if current_movie == movie:\n        current_rating_sum += rating\n        current_rating_count += 1\n    else:\n        if current_movie:\n            rating_average = current_rating_sum / current_rating_count\n            print (\"%s\\t%s\\t%s\" % (current_movie, rating_average, genre))    \n        current_movie = movie\n        current_rating_sum = rating\n        current_rating_count = 1\n\nif current_movie == movie:\n    rating_average = current_rating_sum / current_rating_count\n    print (\"%s\\t%s\\t%s\" % (current_movie, rating_average, genre))\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv 2>/dev/null \\\n    | head -n 5 \\\n    | python ./codes/avgRatingMapper03.py \\\n    | sort \\\n    | python ./codes/avgRatingReducer01.py"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Non-HDFS correctness test"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv 2>/dev/null \\\n    | head -n 2000 \\\n    | python ./codes/avgRatingMapper03.py \\\n    | grep Matrix"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat /data_training/ratings.csv 2>/dev/null \\\n    | head -n 2000 \\\n    | python ./codes/avgRatingMapper03.py \\\n    | grep Matrix \\\n    | sort \\\n    | python ./codes/avgRatingReducer01.py"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "# Manual calculation check via python\n(4.0+1.0+5.0)/3"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Full execution on HDFS"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input /data_training/ratings.csv \\\n    -output intro-to-hadoop/output-movielens-01 \\\n    -file ./codes/avgRatingMapper03.py \\\n    -mapper avgRatingMapper03.py \\\n    -file ./codes/avgRatingReducer01.py \\\n    -reducer avgRatingReducer01.py \\"}, {"cell_type": "markdown", "metadata": {}, "source": "#### 2.1.1 First Error!!!\n\nGo back to the first few lines of the previously and look for the INFO line **Submitted application application_xxxx_xxxx**. Running the logs command of yarn with the provided application ID is a straightforward way to access all available log information for that application. The syntax to view yarn log is:\n\n```\n! yarn logs -applicationId APPLICATION_ID\n```"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "# Run the yarn view log command here\n# Do not run this command in a notebook browser, it will likely crash the browser\n#!yarn logs -applicationId application_1476193845089_0123"}, {"cell_type": "markdown", "metadata": {}, "source": "However, this information is often massive, as it contains the aggregated logs from all tasks (map and reduce) of the job, which can be in the hundreds. The example below demonstrates this problem by displaying all the possible information of a single-task MapReduce job.\nIn this example, the log of a container has three types of log (LogType): \n- stderr: Error messages from the actual task execution\n- stdout: Print out messages if the task includes them\n- syslog: Logging messages from the Hadoop MapReduce operation"}, {"cell_type": "markdown", "metadata": {}, "source": "One approach to reduce the number of possible output is to comment out all non-essential lines (lines containing **INFO**)"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!yarn logs -applicationId application_1505269880969_0056 | grep -v INFO"}, {"cell_type": "markdown", "metadata": {}, "source": "Can we refine the information further:\n- In a MapReduce setting, containers (often) execute the same task.\n- Can we extract only message listing the Container IDs?\n\n~~~\n!yarn logs -applicationId APPLICATION_ID | grep '^Container:'\n~~~"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!yarn logs -applicationId application_1505269880969_0056 | grep '^Container:'"}, {"cell_type": "markdown", "metadata": {}, "source": "Looking at the previous report, we can further identify container information:\n\n```\nContainer: container_XXXXXX on  YYYY.palmetto.clemson.edu_ZZZZZ\n```\n\n- Container ID: container_XXXXXX\n- Address of node where container is placed: YYYY.palmetto.clemson.edu\n\nTo request yarn to provide a more detailed log at container level, we run:\n```\n!yarn logs -applicationId APPLICATION_ID -containerId CONTAINER_ID --nodeAddress NODE_ADDRESS \\\n    | grep -v INFO\n```"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!yarn logs -applicationId application_1505269880969_0056 \\\n    -containerId container_e30_1505269880969_0056_01_000012 \\\n    --nodeAddress dsci035.palmetto.clemson.edu \\\n    | grep -v INFO"}, {"cell_type": "markdown", "metadata": {}, "source": "This error message gives us some insights into the mechanism of Hadoop MapReduce. \n- Where are the map and reduce python scripts located?\n- Where would the *movies.csv* file be, if the *-file* flag is used to upload this file?"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile codes/avgRatingMapper04.py\n#!/usr/bin/env python\n\nimport sys\nimport csv\n\nmovieFile = \"./movies.csv\"\nmovieList = {}\n\nwith open(movieFile, mode = 'r') as infile:\n    reader = csv.reader(infile)\n    for row in reader:\n        movieList[row[0]] = {}\n        movieList[row[0]][\"title\"] = row[1]\n        movieList[row[0]][\"genre\"] = row[2]\n\nfor oneMovie in sys.stdin:\n    oneMovie = oneMovie.strip()\n    ratingInfo = oneMovie.split(\",\")\n    try:\n        movieTitle = movieList[ratingInfo[1]][\"title\"]\n        movieGenre = movieList[ratingInfo[1]][\"genre\"]\n        rating = float(ratingInfo[2])\n        print (\"%s\\t%s\\t%s\" % (movieTitle, rating, movieGenre))\n    except ValueError:\n        continue"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input /data_training/ratings.csv \\\n    -output intro-to-hadoop/output-movielens-01 \\\n    -file ./codes/avgRatingMapper04.py \\\n    -mapper avgRatingMapper04.py \\\n    -file ./codes/avgRatingReducer01.py \\\n    -reducer avgRatingReducer01.py \\\n    -file ./movielens/movies.csv"}, {"cell_type": "markdown", "metadata": {}, "source": "#### 2.1.2 Second Error!!!\n\n- HDFS is read only. Therefore, all output directories must not have existed prior to job submission\n- This can be resolved either by specifying a new output directory or deleting the existing output directory"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input /data_training/ratings.csv \\\n    -output intro-to-hadoop/output-movielens-02 \\\n    -file ./codes/avgRatingMapper04.py \\\n    -mapper avgRatingMapper04.py \\\n    -file ./codes/avgRatingReducer01.py \\\n    -reducer avgRatingReducer01.py \\\n    -file ./movielens/movies.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -ls intro-to-hadoop/output-movielens-02"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat intro-to-hadoop/output-movielens-02/part-00000 \\\n    2>/dev/null | head -n 20"}, {"cell_type": "markdown", "metadata": {}, "source": "### Challenge:\n\n1. Modify *avgRatingReducer02.py* so that only movies with averaged ratings higher than 3.75 are collected\n2. Further enhance your modification so that not only movies with averaged ratings higher than 3.75 are collected but these movies also need to be rated at least 5000 times. "}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile codes/avgRatingMapper04challenge.py\n#!/usr/bin/env python\n\nimport sys\nimport csv\n\nmovieFile = \"./movies.csv\"\nmovieList = {}\n\n\nwith open(movieFile, mode = 'r') as infile:\n    reader = csv.reader(infile)\n    for row in reader:\n        movieList[row[0]] = {}\n        movieList[row[0]][\"title\"] = row[1]\n        movieList[row[0]][\"genre\"] = row[2]\n\nfor oneMovie in sys.stdin:\n    oneMovie = oneMovie.strip()\n    ratingInfo = oneMovie.split(\",\")\n    try:\n        movieTitle = movieList[ratingInfo[1]][\"title\"]\n        movieGenre = movieList[ratingInfo[1]][\"genre\"]\n        rating = float(ratingInfo[2])\n        if _________:\n            print (\"%s\\t%s\\t%s\" % (movieTitle, rating, movieGenre))\n    except ValueError:\n        continue"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "!yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input /repository/movielens/ratings.csv \\\n    -output intro-to-hadoop/output-movielens-challenge \\\n    -file ____________ \\\n    -mapper ___________ \\\n    -file ./codes/avgRatingReducer01.py \\\n    -reducer avgRatingReducer01.py \\\n    -file ./codes/movielens/movies.csv"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 4}