{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# <center> Introduction to Hadoop MapReduce </center>"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. Integrating Hadoop job into Palmetto workflow"}, {"cell_type": "markdown", "metadata": {}, "source": "With `hdp` module, access to Cypress Hadoop cluster can now be invoked within a Palmetto PBS script, allowing the integration of large-data processing components into a standard HPC workflow"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": "%%writefile movieAnalyzer.pbs\n#!/bin/bash\n\n#PBS -N movieData\n#PBS -l select=1:ncpus=8:mem=8gb\n#PBS -l walltime=00:15:00\n#PBS -j oe\n\n# load hdp module and initilalize Keberos tokens\nmodule load hdp/0.1\ncypress-kinit\nklist\n\n# cd into directory containing the PBS script\ncd $PBS_O_WORKDIR\n\n# attempt to remove output directory\nhdfs dfs -rm -r intro-to-hadoop/output-movielens-03\n\n# submit Hadoop job to Cypress\nyarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input /data_training/ratings.csv \\\n    -output intro-to-hadoop/output-movielens-03 \\\n    -file ./codes/avgRatingMapper02.py \\\n    -mapper avgRatingMapper02.py \\\n    -file ./codes/avgRatingReducer02.py \\\n    -reducer avgRatingReducer02.py \\\n    -file ./movielens/movies.csv\n\n# export output data back to Palmetto for further analysis\nhdfs dfs -get intro-to-hadoop/output-movielens-03/part-00000 ."}, {"cell_type": "markdown", "metadata": {}, "source": "Open a terminal, ssh to login001 (DUO required), and submit this script\n\n```\nssh login001\ncd ~/intro-to-hadoop-python\nqsub movieAnalyzer.pbs\n```\n\nView the final output when the job is finished"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!qstat -anu $USER"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!cat part-00000 2>/dev/null | head -n 20"}, {"cell_type": "markdown", "metadata": {}, "source": "## <center> Final Cleanup </center>\n\nExecuting the cell below will clean up all HDFS output directories created as a result of previous MapReduce programs. "}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -ls intro-to-hadoop"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}, "scrolled": true}, "outputs": [], "source": "!hdfs dfs -rm -r intro-to-hadoop/\n!rm -Rf codes/\n!rm -Rf movielens/"}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "jupyter": {"outputs_hidden": true}}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 4}