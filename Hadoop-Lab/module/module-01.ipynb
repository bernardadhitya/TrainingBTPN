{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# <center> Introduction to Hadoop MapReduce </center>"}, {"cell_type": "markdown", "metadata": {}, "source": "Python Jupyter notebook supports execution of Linux command inside the notebook cells. This is done by adding the **!** to the beginning of the command line. It should be noted that each command begins with a **!** will create a new bash shell and close this cell once the execution is done:\n- Full path is required\n- Temporary results and environmental variables will be lost"}, {"cell_type": "markdown", "metadata": {}, "source": "We need to initialize Kerberos authentication mechanism"}, {"cell_type": "markdown", "metadata": {}, "source": "Interaction with Hadoop Distributed File System is done through `hdfs` and its sub-commands"}, {"cell_type": "code", "execution_count": 1, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n\n  OPTIONS is none or any of:\n\n--buildpaths                       attempt to add class files from build tree\n--config dir                       Hadoop config directory\n--daemon (start|status|stop)       operate on a daemon\n--debug                            turn on shell script debug mode\n--help                             usage information\n--hostnames list[,of,host,names]   hosts to use in worker mode\n--hosts filename                   list of hosts to use in worker mode\n--loglevel level                   set the log4j level for this command\n--workers                          turn on worker mode\n\n  SUBCOMMAND is one of:\n\n\n    Admin Commands:\n\ncacheadmin           configure the HDFS cache\ncrypto               configure HDFS encryption zones\ndebug                run a Debug Admin to execute HDFS debug commands\ndfsadmin             run a DFS admin client\ndfsrouteradmin       manage Router-based federation\nec                   run a HDFS ErasureCoding CLI\nfsck                 run a DFS filesystem checking utility\nhaadmin              run a DFS HA admin client\njmxget               get JMX exported values from NameNode or DataNode.\noev                  apply the offline edits viewer to an edits file\noiv                  apply the offline fsimage viewer to an fsimage\noiv_legacy           apply the offline fsimage viewer to a legacy fsimage\nstoragepolicies      list/get/set/satisfyStoragePolicy block storage policies\n\n    Client Commands:\n\nclasspath            prints the class path needed to get the hadoop jar and\n                     the required libraries\ndfs                  run a filesystem command on the file system\nenvvars              display computed Hadoop environment variables\nfetchdt              fetch a delegation token from the NameNode\ngetconf              get config values from configuration\ngroups               get the groups which users belong to\nlsSnapshottableDir   list all snapshottable dirs owned by the current user\nsnapshotDiff         diff two snapshots of a directory or diff the current\n                     directory contents with a snapshot\nversion              print the version\n\n    Daemon Commands:\n\nbalancer             run a cluster balancing utility\ndatanode             run a DFS datanode\ndfsrouter            run the DFS router\ndiskbalancer         Distributes data evenly among disks on a given node\nhttpfs               run HttpFS server, the HDFS HTTP Gateway\njournalnode          run the DFS journalnode\nmover                run a utility to move block replicas across storage types\nnamenode             run the DFS namenode\nnfs3                 run an NFS version 3 gateway\nportmap              run a portmap service\nsecondarynamenode    run the DFS secondary namenode\nsps                  run external storagepolicysatisfier\nzkfc                 run the ZK Failover Controller daemon\n\nSUBCOMMAND may print help when invoked w/o parameters or with -h.\n"}], "source": "!hdfs"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Usage: hadoop fs [generic options]\n\t[-appendToFile <localsrc> ... <dst>]\n\t[-cat [-ignoreCrc] <src> ...]\n\t[-checksum <src> ...]\n\t[-chgrp [-R] GROUP PATH...]\n\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n\t[-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n\t[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]\n\t[-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]\n\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n\t[-df [-h] [<path> ...]]\n\t[-du [-s] [-h] [-v] [-x] <path> ...]\n\t[-expunge [-immediate]]\n\t[-find <path> ... <expression> ...]\n\t[-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-getfacl [-R] <path>]\n\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n\t[-getmerge [-nl] [-skip-empty-file] <src> <localdst>]\n\t[-head <file>]\n\t[-help [cmd ...]]\n\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]\n\t[-mkdir [-p] <path> ...]\n\t[-moveFromLocal [-f] [-p] [-l] [-d] <localsrc> ... <dst>]\n\t[-moveToLocal <src> <localdst>]\n\t[-mv <src> ... <dst>]\n\t[-put [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]\n\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n\t[-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]\n\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n\t[-setfattr {-n name [-v value] | -x name} <path>]\n\t[-setrep [-R] [-w] <rep> <path> ...]\n\t[-stat [format] <path> ...]\n\t[-tail [-f] [-s <sleep interval>] <file>]\n\t[-test -[defswrz] <path>]\n\t[-text [-ignoreCrc] <src> ...]\n\t[-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]\n\t[-touchz <path> ...]\n\t[-truncate [-w] <length> <path> ...]\n\t[-usage [cmd ...]]\n\nGeneric options supported are:\n-conf <configuration file>        specify an application configuration file\n-D <property=value>               define a value for a given property\n-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n-jt <local|resourcemanager:port>  specify a ResourceManager\n-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n\nThe general command line syntax is:\ncommand [genericOptions] [commandOptions]\n\n"}], "source": "!hdfs dfs"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\ndrwxr-xr-x   - nurcahyopujo hadoop          0 2021-04-26 23:27 /data_training\ndrwxr-xr-x   - root         hadoop          0 2021-04-26 13:15 /hbase\ndrwxrwxrwt   - hdfs         hadoop          0 2021-04-24 06:27 /tmp\ndrwxrwxrwt   - hdfs         hadoop          0 2021-04-23 23:21 /user\n"}], "source": "!hdfs dfs -ls /"}, {"cell_type": "markdown", "metadata": {}, "source": "### Challenge\n\nCreate a directory named **intro-to-hadoop** inside your user directory on HDFS"}, {"cell_type": "code", "execution_count": 9, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -mkdir -p /user/nurcahyo/intro-to-hadoop"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\ndrwxr-xr-x   - root hadoop          0 2021-04-26 23:34 /user/nurcahyo/intro-to-hadoop\n"}], "source": "!hdfs dfs -ls /user/nurcahyo"}, {"cell_type": "markdown", "metadata": {}, "source": "### Challenge\n\nUpload the **text** directory into the newly created **intro-to-hadoop** directory. "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!hdfs dfs -put"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -put text intro-to-hadoop"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "### Challenge \n\nCheck the health status of the directories above in HDFS using fsck:\n```\nhdfs fsck <path-to-directory> -files -blocks -locations\n```"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs --config ~/hadoop_palmetto/config fsck intro-to-hadoop/text/complete-shakespeare.txt -files -blocks -locations"}, {"cell_type": "markdown", "metadata": {}, "source": "## MapReduce Programming Paradigm"}, {"cell_type": "markdown", "metadata": {}, "source": "**What is \u201cmap\u201d?**\n\u2013 A function/procedure that is applied to every individual\nelements of a collection/list/array/\u2026\n\n```\nint square(x) { return x*x;}\nmap square [1,2,3,4] -> [1,4,9,16]\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "**What is \u201creduce\u201d?**\n\u2013 A function/procedure that performs an operation on a list.\nThis operation will \u201cfold/reduce\u201d this list into a single value\n(or a smaller subset)\n\n```\nreduce ([1,2,3,4]) using sum -> 10\nreduce ([1,2,3,4]) using multiply -> 24\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "MapReduce is an old concept in functional programming. It is naturally applicable in HDFS: \n- `map` tasks are performed on top of individual data blocks (mainly to filter and decrease raw data contents while increase data value\n- `reduce` tasks are performed on intermediate results from `map` tasks (should now be significantly decreased in size) to calculate the final results. "}, {"cell_type": "markdown", "metadata": {}, "source": "## 1. The Hello World of Hadoop: Word Count"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!mkdir codes"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n    2>/dev/null | head -n 100"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%writefile codes/wordcountMapper.py\n#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python                                          \nimport sys                                                                                                \nfor oneLine in sys.stdin:\n    oneLine = oneLine.strip()\n    for word in oneLine.split(\" \"):\n        if word != \"\":\n            print ('%s\\t%s' % (word, 1)) "}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n    2>/dev/null \\\n    | head -n 20 \\\n    | python ./codes/wordcountMapper.py"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n    2>/dev/null \\\n    | head -n 20 \\\n    | python ./codes/wordcountMapper.py \\\n    | sort"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%writefile codes/wordcountReducer.py\n#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python\nimport sys\n\ncurrent_word = None\ntotal_word_count = 0\n\nfor line in sys.stdin:\n    line = line.strip()\n    word, count = line.split(\"\\t\", 1)\n    try:\n        count = int(count)\n    except ValueError:\n        continue\n    \n    if current_word == word:\n        total_word_count += count\n    else:\n        if current_word:\n            print (\"%s\\t%s\" % (current_word, total_word_count))\n        current_word = word\n        total_word_count = 1\n        \nif current_word == word:\n    print (\"%s\\t%s\" % (current_word, total_word_count))"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat intro-to-hadoop/text/complete-shakespeare.txt \\\n    2>/dev/null \\\n    | head -n 20 \\\n    | python ./codes/wordcountMapper.py \\\n    | sort \\\n    | python ./codes/wordcountReducer.py"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -rm -R intro-to-hadoop/output-wordcount\n!mapred streaming \\\n    -input intro-to-hadoop/text/complete-shakespeare.txt \\\n    -output intro-to-hadoop/output-wordcount \\\n    -file ./codes/wordcountMapper.py \\\n    -mapper wordcountMapper.py \\\n    -file ./codes/wordcountReducer.py \\\n    -reducer wordcountReducer.py"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -ls intro-to-hadoop/output-wordcount"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -cat intro-to-hadoop/output-wordcount/part-00000 \\\n    2>/dev/null | head -n 100"}, {"cell_type": "markdown", "metadata": {}, "source": "### Challenge\n\nModify *wordcountMapper.py* so that punctuations and capitalization are no longer factors in determining unique words"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "%%writefile codes/wordcountEnhancedMapper.py\n#!/software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/bin/python                                          \nimport sys                     \nimport string\n\ntranslator = str.maketrans('', '', string.punctuation)\n\nfor oneLine in sys.stdin:\n    oneLine = oneLine.strip()\n    for word in oneLine.split(\" \"):\n        if word != \"\":\n            newWord = word.translate(translator).lower()\n            print ('%s\\t%s' % (_______, 1)) "}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "!hdfs dfs -rm -R intro-to-hadoop/output-wordcount-enhanced\n!ssh dsciutil yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \\\n    -input intro-to-hadoop/text/gutenberg-shakespeare.txt \\\n    -output intro-to-hadoop/output-wordcount \\\n    -file ____________________________________________________ \\\n    -mapper _____________________ \\\n    -file ____________________________________________________ \\\n    -reducer _____________________ \\"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 4}