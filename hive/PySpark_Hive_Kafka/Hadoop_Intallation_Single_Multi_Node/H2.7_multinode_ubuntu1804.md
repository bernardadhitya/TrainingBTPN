Install java for jre install using ```sudo apt install openjdk-8-jre-headless``` but for running jps install jdk
```
sudo apt-get install openjdk-8-jdk
```

find the ip address of both master and slave
```
ifconfig
```
if command dosent work, install net tools
```
sudo apt install net-tools
```
and then check their ip addresses
```
ifconfig
```

For the easy access to the machines have a host name for the each machine in the etc hosts file.
```
sudo nano /etc/hosts
```

    <ip> <name>
```10.20.3.1 master```

```10.20.3.2 node1```

Let's try to check whether slave can listen to master by pinging it, in master do
```
ping node1
```

Add a hadoop user in both machines.
```
sudo adduser hadoop
```

Login to the master node as hadoop user and generate ssh key pairs.

```
su - hadoop
```
```
ssh-keygen -b 4096
```
specify either 2048 or 4096 (increasing the bits makes it harder to crack the key by brute-force methods

Only make ssh keys in master not in slave. But if you made by mistake on slave, it will be saved as identification and public key, remove them with these commands
```rm /home/hadoop/.ssh/id_rsa.pub```

```rm home/hadoop/.ssh/id_rsa```

Copy the public key to all data nodes and also to master node if you want use that as a datanode.
```
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@master
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@node1
```

if you get the error
ssh: connect t host slave-1 port 22: Connection refused

I'd generally take "connection refused" to mean that the target server isn't listening on port 22.

What's the output of 
```
netstat -an|grep 22 
```
when run on slave-1, it should show 

```tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN```

if not then do on Ubuntu user on both master and slave
```	
sudo apt-get install openssh-server openssh-client
```

try to ssh into slave node thorugh master
```
ssh node1
exit
```

Sometime the error is simple, ssh is not installed. the openssh-client should be installed on your local machine, and openssh-server should be installed on the remote computer. However, you can install both openssh-client and openssh-server on both machines



From Apache hadoop website Download Hadoop binaries not source in hadoop user
```
cd ~
wget http://mirror.olnevhost.net/pub/apache/hadoop/common/hadoop-2.8.1/hadoop-2.8.1.tar.gz
```

NOTE: There is a possibility of below-mentioned error in this setup and installation process.

"hadoop is not in the sudoers file. This incident will be reported." 
This error can be resolved by Login as a root user 

```su ubuntu```

then write
```
sudo adduser hadoop sudo
```

Once file gets downloaded
```
tar -xvf hadoop-2.8.1.tar.gz
mv hadoop-2.8.1.tar.gz hadoop
```

* Note:
The following are the 5 most commonly used options for tar – c, z, f, v, x
v = verbose , display file to compress or uncompress
c = create a new tar file
f = create the tar file with filename provided as the argument
x = extract file
z = use gzip to zip it

being in hadoop user, add hadoop binaries to your path
Add hadoop path to ~/.bashrc  or ~/.profile file, you can list all hidden files in the hadoop user home directory
```
ls
la
nano ~/.bashrc
```
```PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:$PATH```

```
nano ~./profile
```
```PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:$PATH```

```
source ~/.profile
```

Setting up the master node, We can modify the configuration files on master node and replicate them on other machines.

Run this to confirm java version
```
update-alternatives --display java
```
Make sure the path is /usr/lib/jvm/java-8-openjdk-amd64/jre


In hadoop user, Setup JAVA_HOME environment varibale in ~/hadoop/etc/hadoop/hadoop-env.sh 
```
nano ~/hadoop/etc/hadoop/hadoop-env.sh
```

and replace export JAVA_HOME=${JAVA_HOME} in this file
```export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre```

```
source ~/hadoop/etc/hadoop/hadoop-env.sh 
```

Stay in hadoop user.Edit the ~/hadoop/etc/hadoop/core-site.xml. It should contain following xml code.
```
nano ~/hadoop/etc/hadoop/core-site.xml
```

```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://master:9000</value>
        </property>
    </configuration>
```

Change the ~/hadoop/etc/hadoop/hdfs-site.xml
```
nano ~/hadoop/etc/hadoop/hdfs-site.xml
```

```
<configuration>
    <property>
            <name>dfs.namenode.name.dir</name>
            <value>/home/hadoop/data/nameNode</value>
    </property>

    <property>
            <name>dfs.datanode.data.dir</name>
            <value>/home/hadoop/data/dataNode</value>
    </property>

    <property>
            <name>dfs.replication</name>
            <value>1</value>
    </property>
</configuration>
```

Rename the mapred-site.xml.template to mapred-site.xml in ~/hadoop/etc/hadoop directory
Add this to ~/hadoop/etc/hadoop/mapred-site-template.xml
```
cd ~/hadoop/etc/hadoop/
mv mapred-site.xml.template mapred-site.xml
cd ~
nano ~/hadoop/etc/hadoop/mapred-site.xml
```

```
<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
</configuration>
```

Add this to ~/hadoop/etc/hadoop/yarn-site.xml
```
nano ~/hadoop/etc/hadoop/yarn-site.xml
```

```
<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>node-master</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

Configure Slaves, file slaves is used by the startup script to start required daemons on all nodes.
Edit ~/hadoop/etc/hadoop/slaves
```
nano ~/hadoop/etc/hadoop/slaves
```

```node1```

Adding master also as slave because we are that also as data node.

Now, the configuration for hadoop is done.

Let’s copy whole configuration to other machine.
```
cd ~/hadoop
scp -r $(pwd) hadoop@node1:~/hadoop
```

write in hadoop user to check hadoop's version, this will only work if everything till now is done correctly
hadoop version

Running HDFS, Run following to the format name node.
```
hdfs namenode -format
```
Our hadoop configuration is ready to run. Start the dfs by running following command from node-master:
```
start-dfs.sh
```

It’ll start NameNode and SecondaryNameNode on node-master, and DataNode on node1, according to the configuration in the slaves config file.

If you get this error Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured, do 
```
nano ~/hadoop/etc/hadoop/hadoop-env.sh
```
Comment out `export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}` and add

```export HADOOP_HOME=/home/hadoop/hadoop```

```export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop```

source the file.

Check that every process is running with the jps command on each node. You should get on node-master (PID will be different):

```
21922 Jps
21603 NameNode
21787 SecondaryNameNode
```
and on node1
```
19728 DataNode
19819 Jps
```
To stop HDFS on master and slave nodes, run the following command from node-master:
```
stop-dfs.sh
```

You can get useful information about running your HDFS cluster with the hdfs dfsadmin command. Try for example:
```
hdfs dfsadmin -report
```

You can also automatically use the friendlier web user interface. Point your browser to http://localhost:50070 and you’ll get a user-friendly monitoring console.
