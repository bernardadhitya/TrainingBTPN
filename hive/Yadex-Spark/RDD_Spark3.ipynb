{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RDD-Spark3.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV2vo5Qw46_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "#Spark Contexto\n",
        "#df = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvT6yvWr5fAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create RDD example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "rdd = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\n",
        "             (4, 5, 6, 'd e f'),\n",
        "             (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJpyvOgm5vWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyn3LKfk6NNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
        "myData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erPWQVau6WkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Employee = spark.createDataFrame([\n",
        "                        ('1', 'Joe',   '70000', '1'),\n",
        "                        ('2', 'Henry', '80000', '2'),\n",
        "                        ('3', 'Sam',   '60000', '2'),\n",
        "                        ('4', 'Max',   '90000', '1')],\n",
        "                        ['Id', 'Name', 'Sallary','DepartmentId']\n",
        "                       ).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9ErDNp6stk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/GCPBigData/Yadex-Spark/blob/master/folha.csv\n",
        "## set up  SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create RDD example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read.load(\"/content/folha.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")                \n",
        "\n",
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9hbmtQ59veF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## set up  SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .appName(\"Python Spark create RDD example\") \\\n",
        "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "## User information\n",
        "user = 'your_username'\n",
        "pw   = 'your_password'\n",
        "\n",
        "## Database information\n",
        "table_name = 'table_name'\n",
        "url = 'jdbc:postgresql://##.###.###.##:5432/dataset?user='+user+'&password='+pw\n",
        "properties ={'driver': 'org.postgresql.Driver', 'password': pw,'user': user}\n",
        "\n",
        "df = spark.read.jdbc(url=url, table=table_name, properties=properties)\n",
        "\n",
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1abnP1Bd90_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.conf import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import HiveContext\n",
        "\n",
        "sc= SparkContext('local','example')\n",
        "hc = HiveContext(sc)\n",
        "tf1 = sc.textFile(\"hdfs://cdhstltest/user/data/demo.CSV\")\n",
        "print(tf1.first())\n",
        "\n",
        "hc.sql(\"use intg_cme_w\")\n",
        "spf = hc.sql(\"SELECT * FROM spf LIMIT 100\")\n",
        "print(spf.show(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxyKLV-k_GMt",
        "colab_type": "text"
      },
      "source": [
        "### There are two ways to create the RDD\n",
        "\n",
        "Parallelizing an existing collection in your driver program.\n",
        "Or referencing a dataset in an external storage system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL-k4fK4_EYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create RDD example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "import numpy as np\n",
        "data = np.arange(1,100)\n",
        "print(data[90:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYkuS6f4ABnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#paralelizando a coleção existente\n",
        "distData = sc.parallelize(data)\n",
        "\n",
        "#também podemos especificar o número de partições para cortar o conjunto de dados\n",
        "distData_ten = sc.parallelize(data,10)\n",
        "\n",
        "distData.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xtLuYUSANeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#recuperar todos os resultados\n",
        "distData.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMcX64VQD8Ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#retrieve first n elements from RDD\n",
        "distData.take(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELvd7Rj4EFy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the result\n",
        "rdd = sc.parallelize(range(1, 400)).map(lambda x: (x, x * x))\n",
        "rdd.saveAsSequenceFile(\"/content/sparktest4.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}