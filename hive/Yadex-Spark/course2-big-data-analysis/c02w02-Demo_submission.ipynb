{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The demonstrative notebook for Hive assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run any HiveQL query in the notebook you should:\n",
    "1. write the code of query into a separate file using `%%writefile [-a] <file>` magic,\n",
    "2. execute this file in hive using `! hive -f <file>` command.\n",
    "\n",
    "To make grading system check a task correctly, execution command must be in a separate cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creation the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, create your Hive database. You can name the database whatever you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop database if it has already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile creation_db.hql\n",
    "\n",
    "DROP DATABASE IF EXISTS demodb CASCADE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a creation_db.hql\n",
    "CREATE DATABASE demodb LOCATION '/user/jovyan/somemetastore';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the file we filled earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.062 seconds\n",
      "OK\n",
      "Time taken: 0.524 seconds\n"
     ]
    }
   ],
   "source": [
    "! hive -f creation_db.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the real Hadoop-cluster where your submission will be checked we already have precreated Hive databases for all users. This helps to avoid database name conflicts. If you're the new user, the database will be created during your first submission of Hive assignment. The system won't allow you to create your own database on Hadoop-cluster so when you submit the final version of the task you shoud **remove or comment** all the lines related to database's dropping and creation. \n",
    "\n",
    "You can left all the lines with `USE` without any changes. The grading system will replace database's name to name of the precreated database. In assignments 2 and 3 you'll need to use `stackoverflow_` database. This database's name will not be changed by the grading system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation the external table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us our source dataset have 2 collumns:\n",
    "* ip-address,\n",
    "* its subnet's mask.\n",
    "\n",
    "For example:\n",
    "```\n",
    "148.45.113.216\t255.255.255.248\n",
    "203.98.141.0\t255.255.255.240\n",
    "183.168.36.0\t255.255.255.128\n",
    "111.157.172.232\t255.255.255.248\n",
    "80.46.87.0\t255.255.255.0\n",
    "247.248.233.0\t255.255.255.128\n",
    "```\n",
    "Now we'll create the external table with 2 fields: ip and mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exteral_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile exteral_table.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS Subnets;\n",
    "\n",
    "CREATE EXTERNAL TABLE Subnets (\n",
    "    ip STRING,\n",
    "    mask STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY  '\\t'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/data/subnets/ips';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 1.071 seconds\n",
      "OK\n",
      "Time taken: 0.11 seconds\n",
      "OK\n",
      "Time taken: 0.93 seconds\n"
     ]
    }
   ],
   "source": [
    "! hive -f exteral_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Demo query on created table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a simpe query:\n",
    " > Compute avarage value of IPs for each subnet's mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "USE demodb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a query.hql\n",
    "\n",
    "SELECT AVG(counts.cnt)\n",
    "FROM (\n",
    "    SELECT mask, count(ip) as cnt\n",
    "    FROM Subnets\n",
    "    GROUP BY mask\n",
    ") counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 1.052 seconds\n",
      "Query ID = jovyan_20181113110505_cc5d3ac7-501f-4915-9d78-ffd05f07875d\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1542041219823_0001, Tracking URL = http://c5bd5fec03a1:8088/proxy/application_1542041219823_0001/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1542041219823_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2018-11-13 11:05:51,011 Stage-1 map = 0%,  reduce = 0%\n",
      "2018-11-13 11:05:58,711 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.66 sec\n",
      "2018-11-13 11:06:07,418 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.5 sec\n",
      "MapReduce Total cumulative CPU time: 6 seconds 500 msec\n",
      "Ended Job = job_1542041219823_0001\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1542041219823_0002, Tracking URL = http://c5bd5fec03a1:8088/proxy/application_1542041219823_0002/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1542041219823_0002\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2018-11-13 11:06:25,599 Stage-2 map = 0%,  reduce = 0%\n",
      "2018-11-13 11:06:33,116 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.36 sec\n",
      "2018-11-13 11:06:42,719 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 5.83 sec\n",
      "MapReduce Total cumulative CPU time: 5 seconds 830 msec\n",
      "Ended Job = job_1542041219823_0002\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.5 sec   HDFS Read: 13758 HDFS Write: 127 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 5.83 sec   HDFS Read: 5002 HDFS Write: 19 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 12 seconds 330 msec\n",
      "OK\n",
      "35.714285714285715\n",
      "Time taken: 73.427 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take into account that the grading system catch all output (both result and MapReduce logs) from the last cell of the notebook, so **don't** redirect any output from this cell to `/dev/null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
