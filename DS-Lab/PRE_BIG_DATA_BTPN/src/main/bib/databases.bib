Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Redmon2012,
address = {Dallas, Texas},
author = {Redmond, Eric and Wilson, Jim R.},
edition = {P1.0},
editor = {Carter, Jacquilyn},
file = {:home/sphelps/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Redmond, Wilson - 2012 - Seven Databases in Seven Weeks A Guide to Modern Databases and the NoSQL Movement.pdf:pdf},
isbn = {9781934356920},
pages = {347},
publisher = {The Pragmatic Bookshelf},
title = {{Seven Databases in Seven Weeks: A Guide to Modern Databases and the NoSQL Movement}},
year = {2012}
}
@article{Yang2015,
abstract = {Accurate real-time tracking of influenza outbreaks helps public health officials make timely and meaningful decisions that could save lives. We propose a new influenza tracking model, ARGO (AutoRegression with GOogle search data), that uses publicly available online search data. In addition to having a rigorous statistical foundation, ARGO outperforms all previously available tracking models, including the latest version of Google Flu Trends (GFT), even though it uses only low-quality search data as input from publicly available Google Trends and Google Correlate websites. ARGO not only incorporates the seasonality in influenza epidemics, but also captures changes in people's online search behavior over time. ARGO is also flexible, self-correcting, robust and scalable, making it a potentially powerful tool that can be used for real-time tracking of other social events at multiple temporal and spatial resolutions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.0086},
author = {Yang, Shihao and Santillana, Mauricio and Kou, S C},
doi = {10.1073/pnas.1515373112},
eprint = {arXiv:1505.0086},
file = {:home/sphelps/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Santillana, Kou - 2015 - Accurate estimation of influenza epidemics using Google search data via ARGO.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Science},
keywords = {autoregressive-exogenous model,big data,digital disease detection,google flu trends,illnesses activity,l 1 regu-,real-time estimation of influenza-like,seasonal influenza},
pages = {1--6},
title = {{Accurate estimation of influenza epidemics using Google search data via ARGO}},
year = {2015}
}
@book{McKinney2012,
author = {McKinney, W.},
publisher = {O'Reilly},
title = {{Python for Data Analysis}},
year = {2012}
}
@book{Elmasri2013,
author = {Elmasri, R. and Navethe, S.},
edition = {sixth},
publisher = {Pearson},
title = {{Fundamentals of Database Systems}},
year = {2013}
}
@article{Bloom1970,
abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
author = {Bloom, Burton H.},
doi = {10.1145/362686.362692},
file = {:home/sphelps/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bloom - 1970 - Spacetime trade-offs in hash coding with allowable errors.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage,storage layout,touch},
mendeley-tags = {touch},
number = {7},
pages = {422--426},
title = {{Space/time trade-offs in hash coding with allowable errors}},
volume = {13},
year = {1970}
}
@book{Stallings2016,
author = {Stallings, William},
edition = {10th},
publisher = {Pearson},
title = {{Computer Organization and Architecture}},
year = {2016}
}
@book{Karau2015,
author = {Karau, H. and Wendell, P. and Zaharia, M.},
publisher = {O'Reilly},
title = {{Learning Spark: Lightning-Fast Big Data Analysis}},
year = {2015}
}
@book{Lambert2013,
author = {Lambert, K. A.},
publisher = {Delmar Cengage Learning},
title = {{Fundamentals of Python: Data Structures}},
year = {2013}
}
@book{George2011,
author = {George, Lars},
edition = {1st},
isbn = {978-1449396107},
pages = {556},
publisher = {O'Reilly Media},
title = {{HBase: The Definitive Guide}},
year = {2011}
}
@book{Immon2005,
author = {Immon, W. H.},
publisher = {Wiley},
title = {{Building the Data Warehouse}},
year = {2005}
}
@article{Stonebraker1986,
abstract = {There are three dominent themes in building high transaction rate multiprocessor systems, namely shared memory (e.g. Synapse, IBM/AP configurations), shared disk (e.g. VAX/cluster, any multi-ported disk system), and shared nothing (e.g. Tandem, Tolerant). This paper argues that shared nothing is the pre- ferred approach.},
author = {Stonebraker, Michael},
file = {:home/sphelps/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stonebraker - 1986 - The Case for Shared Nothing.pdf:pdf},
issn = {09547762},
journal = {IEEE Database Eng. Bull.},
number = {1},
pages = {4--9},
title = {{The Case for Shared Nothing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.5370{\&}rep=rep1{\&}type=pdf},
volume = {9},
year = {1986}
}
@book{Mayer-Schonberger2013,
address = {London},
author = {Mayer-Sch{\"{o}}nberger, Viktor and Cukier, Kenneth},
edition = {1st},
isbn = {9781848547933},
publisher = {John Murray},
title = {{Big Data: A Revolution That Will Transform How We Live, Work and Think}},
year = {2013}
}
@book{Grus2015,
author = {Grus, J.},
publisher = {O'Reilly},
title = {{Data Science from Scratch: First Principles with Python}},
year = {2015}
}
@book{Model2015,
author = {Model, M. L.},
publisher = {O'Reilly},
title = {{Bioinfortmatics Programming Using Python}},
year = {2015}
}
